"""
ë…¼ë¬¸ ë¹„êµ ë¶„ì„ ëª¨ë“ˆ
"""

from pathlib import Path
from typing import Optional, Dict, Any, List
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


def compare_papers(
    paper_path: str,
    draft_path: str,
    output_path: Optional[str] = None
) -> str:
    """ë…¼ë¬¸ê³¼ ì´ˆì•ˆì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

    Args:
        paper_path: ì°¸ì¡° ë…¼ë¬¸ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        draft_path: ì‚¬ìš©ì ì´ˆì•ˆ íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)

    Returns:
        ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    paper_path = Path(paper_path).resolve()
    draft_path = Path(draft_path).resolve()

    if not paper_path.exists():
        raise FileNotFoundError(f"ë…¼ë¬¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {paper_path}")
    if not draft_path.exists():
        raise FileNotFoundError(f"ì´ˆì•ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {draft_path}")

    # ì¶œë ¥ ê²½ë¡œ ê²°ì •
    if output_path:
        output_path = Path(output_path).resolve()
    else:
        output_path = Path.cwd() / 'logs' / 'analysis' / f"comparison_{paper_path.stem}.md"

    # í…ìŠ¤íŠ¸ ë¡œë“œ
    paper_text = paper_path.read_text(encoding='utf-8')
    draft_text = draft_path.read_text(encoding='utf-8')

    # ë¹„êµ ë¶„ì„ ìˆ˜í–‰
    analysis = _analyze_comparison(paper_text, draft_text, paper_path.name, draft_path.name)

    # ê²°ê³¼ ì €ì¥
    content = _format_comparison_report(analysis)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content, encoding='utf-8')

    return str(output_path)


def _analyze_comparison(
    paper_text: str,
    draft_text: str,
    paper_name: str,
    draft_name: str
) -> Dict[str, Any]:
    """ë‘ í…ìŠ¤íŠ¸ë¥¼ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤."""

    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    paper_clean = _preprocess_text(paper_text)
    draft_clean = _preprocess_text(draft_text)

    # TF-IDF ìœ ì‚¬ë„ ê³„ì‚°
    similarity = _calculate_similarity(paper_clean, draft_clean)

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    paper_keywords = _extract_keywords(paper_clean)
    draft_keywords = _extract_keywords(draft_clean)

    # ê³µí†µ í‚¤ì›Œë“œ ë° ì°¨ì´ì 
    common_keywords = set(paper_keywords) & set(draft_keywords)
    paper_unique = set(paper_keywords) - set(draft_keywords)
    draft_unique = set(draft_keywords) - set(paper_keywords)

    # ì¸ìš© ê°€ì¹˜ ì ìˆ˜ ê³„ì‚°
    citation_score = _calculate_citation_score(similarity, common_keywords, paper_keywords)

    return {
        'paper_name': paper_name,
        'draft_name': draft_name,
        'similarity': similarity,
        'citation_score': citation_score,
        'citation_grade': _get_citation_grade(citation_score),
        'paper_keywords': paper_keywords[:20],
        'draft_keywords': draft_keywords[:20],
        'common_keywords': list(common_keywords)[:15],
        'paper_unique_keywords': list(paper_unique)[:10],
        'draft_unique_keywords': list(draft_unique)[:10],
        'paper_length': len(paper_text.split()),
        'draft_length': len(draft_text.split()),
    }


def _preprocess_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()
    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ì•ŒíŒŒë²³, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
    text = re.sub(r'[^a-z0-9ê°€-í£\s]', ' ', text)
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    return text.strip()


def _calculate_similarity(text1: str, text2: str) -> float:
    """ë‘ í…ìŠ¤íŠ¸ì˜ TF-IDF ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    try:
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        return round(float(similarity), 4)
    except Exception:
        return 0.0


def _extract_keywords(text: str, top_n: int = 30) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        vectorizer = TfidfVectorizer(
            max_features=top_n * 2,
            stop_words='english',
            ngram_range=(1, 2)
        )
        tfidf_matrix = vectorizer.fit_transform([text])
        feature_names = vectorizer.get_feature_names_out()
        scores = tfidf_matrix.toarray()[0]

        # ì ìˆ˜ë¡œ ì •ë ¬
        keyword_scores = list(zip(feature_names, scores))
        keyword_scores.sort(key=lambda x: x[1], reverse=True)

        return [kw for kw, score in keyword_scores[:top_n] if score > 0]
    except Exception:
        return []


def _calculate_citation_score(
    similarity: float,
    common_keywords: set,
    paper_keywords: List[str]
) -> float:
    """ì¸ìš© ê°€ì¹˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

    ì ìˆ˜ ê¸°ì¤€:
    - ìœ ì‚¬ë„ (40%): í…ìŠ¤íŠ¸ ìœ ì‚¬ì„±
    - í‚¤ì›Œë“œ ì¤‘ë³µë¥  (40%): ê³µí†µ í‚¤ì›Œë“œ ë¹„ìœ¨
    - ë…¼ë¬¸ í’ˆì§ˆ ì¶”ì • (20%): í‚¤ì›Œë“œ ë‹¤ì–‘ì„±
    """
    # ìœ ì‚¬ë„ ì ìˆ˜ (0-1)
    similarity_score = min(similarity * 1.5, 1.0)  # ì•½ê°„ ìŠ¤ì¼€ì¼ ì¡°ì •

    # í‚¤ì›Œë“œ ì¤‘ë³µë¥  (0-1)
    if paper_keywords:
        overlap_ratio = len(common_keywords) / len(paper_keywords)
    else:
        overlap_ratio = 0.0

    # í’ˆì§ˆ ì¶”ì • (í‚¤ì›Œë“œ ìˆ˜ ê¸°ë°˜)
    quality_score = min(len(paper_keywords) / 20, 1.0)

    # ê°€ì¤‘ í‰ê· 
    final_score = (
        similarity_score * 0.4 +
        overlap_ratio * 0.4 +
        quality_score * 0.2
    )

    return round(final_score, 2)


def _get_citation_grade(score: float) -> Dict[str, str]:
    """ì¸ìš© ê°€ì¹˜ ë“±ê¸‰ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if score >= 0.7:
        return {
            'grade': 'Must',
            'emoji': 'ğŸ”´',
            'description': 'í•„ìˆ˜ ì¸ìš© - í•µì‹¬ ë°©ë²•ë¡  ì œê³µ, ì§ì ‘ì  ê´€ë ¨ì„±'
        }
    elif score >= 0.4:
        return {
            'grade': 'Optional',
            'emoji': 'ğŸŸ¡',
            'description': 'ì„ íƒì  ì¸ìš© - ê´€ë ¨ ê¸°ë²• ì†Œê°œ, ë¹„êµ ëŒ€ìƒ'
        }
    else:
        return {
            'grade': 'Avoid',
            'emoji': 'âšª',
            'description': 'ì¸ìš© ë¶ˆí•„ìš” - ê´€ë ¨ì„± ë‚®ìŒ, ë” ë‚˜ì€ ëŒ€ì•ˆ ì¡´ì¬'
        }


def _format_comparison_report(analysis: Dict[str, Any]) -> str:
    """ë¹„êµ ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í¬ë§·í•©ë‹ˆë‹¤."""
    grade = analysis['citation_grade']

    report = f"""# ë…¼ë¬¸ ë¹„êµ ë¶„ì„ ê²°ê³¼

## ë¹„êµ ëŒ€ìƒ

| í•­ëª© | ë‚´ìš© |
|------|------|
| ì°¸ì¡° ë…¼ë¬¸ | {analysis['paper_name']} |
| ì‚¬ìš©ì ì´ˆì•ˆ | {analysis['draft_name']} |
| ë…¼ë¬¸ ê¸¸ì´ | {analysis['paper_length']} ë‹¨ì–´ |
| ì´ˆì•ˆ ê¸¸ì´ | {analysis['draft_length']} ë‹¨ì–´ |

## ìœ ì‚¬ë„ ë¶„ì„

| ì§€í‘œ | ê°’ |
|------|------|
| í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ | {analysis['similarity']:.2%} |
| ì¸ìš© ê°€ì¹˜ ì ìˆ˜ | {analysis['citation_score']:.2f} |

## ì¸ìš© ê°€ì¹˜ íŒë‹¨

### ë“±ê¸‰: {grade['grade']} {grade['emoji']}

**ì ìˆ˜**: {analysis['citation_score']:.2f} / 1.00

**íŒë‹¨**: {grade['description']}

## í‚¤ì›Œë“œ ë¶„ì„

### ê³µí†µ í‚¤ì›Œë“œ (ê´€ë ¨ì„± ì§€í‘œ)
{', '.join(analysis['common_keywords']) if analysis['common_keywords'] else 'ê³µí†µ í‚¤ì›Œë“œ ì—†ìŒ'}

### ë…¼ë¬¸ ê³ ìœ  í‚¤ì›Œë“œ (ì°¸ê³  ê°€ì¹˜)
{', '.join(analysis['paper_unique_keywords']) if analysis['paper_unique_keywords'] else 'ì—†ìŒ'}

### ì´ˆì•ˆ ê³ ìœ  í‚¤ì›Œë“œ (ì°¨ë³„ì )
{', '.join(analysis['draft_unique_keywords']) if analysis['draft_unique_keywords'] else 'ì—†ìŒ'}

## í™œìš© ê¶Œì¥ ì‚¬í•­

"""

    if grade['grade'] == 'Must':
        report += """- âœ… **Related Work** ì„¹ì…˜ì— ë°˜ë“œì‹œ í¬í•¨
- âœ… ë°©ë²•ë¡  ë¹„êµ ë¶„ì„ì— í™œìš©
- âœ… í•µì‹¬ ìˆ˜ì‹ ë° ê°œë… ì¸ìš© ê¶Œì¥
"""
    elif grade['grade'] == 'Optional':
        report += """- ğŸ”¶ **Related Work** ì„¹ì…˜ì— ì„ íƒì  í¬í•¨
- ğŸ”¶ íŠ¹ì • ê¸°ë²• ë¹„êµ ì‹œ ì°¸ì¡°
- ğŸ”¶ ë°°ê²½ ì§€ì‹ ì œê³µ ìš©ë„ë¡œ í™œìš©
"""
    else:
        report += """- âšª ì¸ìš© ìš°ì„ ìˆœìœ„ ë‚®ìŒ
- âšª ë” ê´€ë ¨ì„± ë†’ì€ ë…¼ë¬¸ ê²€ìƒ‰ ê¶Œì¥
- âšª í•„ìš”ì‹œ ê°ì£¼ ë˜ëŠ” ë¶€ë¡ì—ì„œ ì–¸ê¸‰
"""

    report += """
---
*ì´ ë¶„ì„ì€ Scholar-Syncì— ì˜í•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""

    return report


if __name__ == '__main__':
    import sys
    if len(sys.argv) < 3:
        print("ì‚¬ìš©ë²•: python compare_papers.py <paper_path> <draft_path> [output_path]")
        sys.exit(1)

    paper_file = sys.argv[1]
    draft_file = sys.argv[2]
    output_file = sys.argv[3] if len(sys.argv) > 3 else None

    result = compare_papers(paper_file, draft_file, output_file)
    print(f"ë¹„êµ ë¶„ì„ ì™„ë£Œ: {result}")